GN, 2014 - my comments on NEMEX_F


MAY, 2014:

Start implementing Faerie as part of nemex.f. Already implemented by Amir:
- inverted index structure for Faerie for char and token based approach
- nemex-f configuration under /nemexa/src/main/webapp/resources/configurations.xml
	propertye "nemex-f"
	
- adapt/extend xml configuration 
	- pruning function
	- char/token
	- output function

Steps to do:
- define class NemexFIndex
	- keeps the dictionary and its main parameters: sim funct, ngram etc.
	- load and store gazetteer as done in NEMEX_A
		- note: it is required that inverted list of a ngram-key are naturally ordered, 
					which seems to be the case
	
- then define class NemexFContainer.java
	- mantains the state of an extraction session

	ngram-list: ngram representation of input text; 
		can use type de.dfki.lt.nemex.a.ngram.CharacterNgram
	ngram-list-length: 
		is defined by above
  	inverted-lists: a list of lists of Long or NIL
  		index follows ngram-list
  	position-list: actually a hash
  		a mapping between top elements of inverted-lists and their position in the ngram-list 
  		this actually indicates which ngrams of an entry of the dictionary are aligned in the input ngram list
  	min-heap
  		these top-elements are also stored in the min heap
  		
  		- find java library for Fibonacci min heap -> 
			I think import java.util.PriorityQueue; 
			should do it because it already implements a min-heap as default
			see also how it is used in JTIG
	
	
  	toplevel-element
  		the poped element of the heap
  	count-array
  		the array that records an entity's occurrence number for each possible substring of the document
  		in my Lisp version I am using a hashmap from spans to integers which records the occurrence of an entity
  		e at all positions in ngram-list
  		thus the spans correspond to candidate substrings in the input that eventually are similar with e
  	valid-substring-interval
  		store the lower/upper bounds for current entity
  		has to be computed online with the used sim function
  	lower-entity-length
  		T_l
  		has to be computed online with the used sim function
 
 
 - define NemexFController
 	- define major search strategies:
 		lazy-count-pruning
		initialize-entity-boundaries
		call BUCKET or BINARY pruning
		get-candidate-pairs
 		
 - define NemexFMain:
 	initialization 
 		dictionary
 		document
 		container
 	main loop over heap: 
 	- initialization of NEMEX_F object for given input text
 	- main loop over heap: for each selected entity e do:
 		- initialization of count-array
 		- main steps then
 			lazy-count-pruning
			initialize-entity-boundaries
			call BUCKET or BINARY pruning
			get-candidate-pairs
			adjust NEMEX_F object
		- adjust NEMEX_F means
			- delete e from inverted lists
			- select new top level elements
			- create or update position lists
			
25th AUGUST, 2014:

Start implementation of Faerie.

For the ngram representation of the input I need to make sure that the ngram list is position preserving.
Thus, I need to use /nemexa/src/main/java/de/dfki/lt/nemex/a/ngram/CharacterNgramWithDuplicate.java
And hence make sure that <ignoreDuplicateNgrams>false</ignoreDuplicateNgrams> !

In LispVersion I preprocess the ngram-list and its length is stored in internal entry
In NemexA this is NOT done. So, I need to compute the ngram-lenght of an entry online ! 
It is possible, but check equation.

SOLUTION:
In general, a string x consisting of |X| letters yields (|x| + n − 1) elements of ngrams, cf. Coling 2010.
However, I need to substract additional ( 2 * (n - 1)), because I do not use start/end ngram.
(|x| + (n − 1) - ( 2 * (n - 1))) = (|x| - n + 1)

OBACHT:
IDs of lexical entries start from 1 in the dictionary, but are stored as ID-1 in the inverted index.
So, make sure to retrieve entries in the gazetters via invertedIndexElement+1!

NOTE:
I will start with 0 for document ! (different as in paper)

NOTE:
in paper document starts at index i=1, and len=l
in my implementation: i=0, len=l+1

SIMILARITY FUNCTIONS:
Have to be reimplemented because NemexA is tuned for input ngram only; 
it means it access entityLength from ngram size when it constructs index, but then does not store it with entity.
Simialrity functions compute basically for online-constructed ngrams
But for Faerie ngram length have to be offline !
Muss also package kopieren und anpassen

NOTE:
I do the initialization of the entity boundaries before calling the pruning strategies.

NOTE:
I explicitly avoid duplicate entities in minHeap, because otherwise, duplicate candidates are created.
I think this is correct, because the position list takes care of the occurrence of ngrams of an entity
in a string.

NOTE on sorting:
position list must be sorted, but adjustNemexFContainer does not guarantee it; so make sure pruner is doing

NOTE:
I should precompute ngram length of dictionary entries when loading a dictionary and
create a hashmap. This would allow me to remove entity content from memory, and only store index and length;

Problems:

current :no version chrashes for large example.
It seems that inverted index list keeps some elements at least twice.
It seems to be caused by removing position list of top element !
	Without removing it works, but it seems to be not exhaustive.
	Removal here is bad because I do not sort ! So need to do sort, because
	for pruning I have to do it anyway
A single entry as query is not found. 
	-> caused by adjustment of container, because cause to early termination
	-> also had to adapt initialContainer
Now, it seems that get candidates is too restricted if threshold is > 0.9
because then ngramLen=min=max -> check this case
seems to be ok in getCandidates, where I use >= instead of >

OK< let me first print the found pairs, so that I can see whether results are correct !
	-> Take care about proper computation of right span!
Then, item counter and candidate counter with time.
	-> keep an item counter for the spanHash in countOccurrenceOfentityAtithPosition
	
9th SEPTEMBER, 2014: 

first version for :no and :lazy cases implemented and tested

1. define pruning interface -> OK
2. integrate no and lazy strategies -> OK
3. Initialize aligner when creating the controller object -> OK

4. similarity functions

- cosine -> OK -> double-checked
- dice -> OK -> double-checked
- jaccard -> OK -> double checked
- ed OK -> threshold TAU should be a double encoding 
			an integer which actually encodes the number of allowable edit operations
			HIERIX: results look a bit strange
		-> double-checked
- eds -> OK -> -> double-checked


18th SEPTEMBER, 2014:
5. bucket -> OK

21th SEPTEMBER, 2014:
6. binary -> OK
	- first version runs
	- got same pairs as for bucket for small documents
	- but too many for more complex examples 
	- also, for small documents computes more items than for bucket, which should not be the case
	-> Do I count too much ?
	
-> check it again carefully ->
	- if I do not use binaryInputShift, I get the same result when just using i=i+1
		- I guess it means that binaryInputShift is OK
		
-> I do not see what is wrong with binary, maybe here
		de.dfki.lt.nemex.f.aligner.BinaryCountPruneAligner.enumerateCandidateWindows(List<Integer>, NemexFContainer)
		where I could do a more restricted min/max for enumerating candidates;
		however, there are cases were distance is lower than LO for where it is not with bucket.

		
29th SEPTEMBER
- implemented tighterUpperWindowSize() for similarity function interface
	for dice, jaccard, cosine, see 2012-Faerie page 534
- use NemexFContainer.computeUpperWindowSize as interface to the sim fct for selecting best upper bound fct.

8th OCTOBER
- 	when running binary count with different sim fct, I get different result pairs, 
	for Jaccard and EDS compared to Cosine and Dice
	
-> correct - yes, should be because different definition of similarity fcts

MOVE nemexa to gitHub ?

7. improve memory footprint for dictionary
	- currently I need to load whole word-strings because I compute ngram length of entities online
	
	- add it to dictionary when loading, and then remove information not needed to keep in memory
		- keep own hash table with mapping ID -> ngramLength
		- build when dictionary is loaded; should also be possible to make it persisted

8. IMPROVEMENT:

MAY, 2015

- post filtering of "wrong" substrings, i.e., matched substrings that are in the wrong context, e.g.,
	- "present" -> "ent" matches because it is a dictionary entry
	- how to "learn" post-filters ?
	
- foundCandidates: (check whether correct)
	- it is an inverted index to entities e
	- collections of start positions
		- collections of length positions
	- for that reason: e should be one single element
		- start and len are key sets
		- CHANGED MAY 2015
	
- ranking of candidates
	- according to paper page 536, advantage of binary pruning
		- the best similar pairs are those which share as many tokens as possible
	  	- group all candidates based on the same number of tokens
	  	- consider a group G_g with g tokens
	  	- let T_g be a threshold computed on basis of |e| and g
	  		 -> How exactly is this computed ?
	  	- then we prune all elements in group G_g, if |P_e[pi, ..., pj]| < T_g
	- I think, I have that as part of my foundCandidates

- selecting best matches:
	- note, that under index left->[len_i, ..., len_j], I find same entity e !
	- instead of returning each substring [left, len_i] individually, I can return substring:
		- [left, len_j] -> gives max length
		- [left, floor(len_j, len_i)] -> gives middle length -> most closest match
	-> DONE
		
	- if there is a chain/bucket of left elements left_1,left_2,..,leftK...
		- [left1, max-len-element(left_1,left_2,..,leftK)] 
			-> gives maximum window size of entity e in document
	-> TODO: length still not computed correctly, because I have to consider distance between 
		start and max len of a bucket to get the max len
	-> DONE
	- similar for middle left-element
		- [floor(left_1,left_2,..,leftK), middle-len-element(left_1,left_2,..,leftK)] 
	-> DONE

- DONE: check and update data structure foundCandidates
	-> List(Long) is not required

- DONE: Integration of LOG file


- DONE: define specific class for via CandidateSelector

-> QUESTION: can one compute CandidateSelector online ?

HIERIX May 2015:
- evaluation:
	- define resetting of controller -> DONE
	- controllers can now run in parallel on same dictionary and ngram of input string
		-> NOTE REALLY, because single global NemexFDataObject binds aligner and selector
		-> check what slots of NemexFDataObject are really necessary
-> DONE: defined NemexFBean for us instead of static NemexFDataObject
	-> Should now be possible to run NemexF on a cluster for same dictionary and distributed file
	
- improve lexicon storage
-> will create own NEMEXF gazetter class

- make own project ?

- can I use better data structure than hashMap ? 

- why does binary bucket compute more candidate pairs than bucket and lazy 
	-> need to do automatic evaluation to compare results
	-> do this, once I have implemented my post-filtering
-> with post-filtering it looks much better, especially when using MiddleBucket
-> anyway: it seems I am computing too many candidates; maybe binary branch is wrong at some point

- other similarity functions
- evaluation
- demo-page which shows how NemexF is running
- restful API and showing streaming based extraction

ERROR ?

when I use this
NemexFDataObject.nGramSize = 1;
NemexFDataObject.similarityMeasure = SimilarityMeasure.ED_SIMILARITY_MEASURE;
NemexFDataObject.similarityThreshold = 1;

I get strange results, e.g., [36,46]: [3]; membership; [-9.197762, venkatesh, NG:1:-9.197762]